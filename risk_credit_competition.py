# -*- coding: utf-8 -*-
"""Risk Credit Competition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Utoxg9-NkEz_3EeKvcGT0RCpejqKOS5
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn import metrics

df_train = pd.read_csv('/content/train.csv')
df_test = pd.read_csv('/content/test.csv')
df_train.head()

"""Data exploratory"""

df_train.shape

print(df_train.info())
df_train.nunique()

#Replace Missing Value
df_train = df_train.dropna(axis=0)
df_train.isnull().sum()

df_test = df_test.dropna(axis=0)
df_test.isnull().sum()

#change customer_bod to age
df_train['customer_bod'] = pd.to_datetime(df_train['customer_bod'])
today = pd.to_datetime('today')
df_train['customer_bod'] = today.year - df_train['customer_bod'].dt.year
df_train = df_train.rename(columns={'customer_bod': 'age'})
df_train.head()

df_test['customer_bod'] = pd.to_datetime(df_test['customer_bod'])
today = pd.to_datetime('today')
df_test['customer_bod'] = today.year - df_test['customer_bod'].dt.year
df_test = df_test.rename(columns={'customer_bod': 'age'})
df_test.head()

# Create dummy variables, contain 0 or 1 as a result of one hot encoding depending on whether value is True
df_train['gender'] = df_train['gender'].map({'Female':1, 'Male':2})
df_train['student'] = df_train['student'].map({'Yes':1, 'No':2})
df_train['employment'] = df_train['employment'].map({'Self Employed':1, 'Salaried':2})
df_train.head()

df_test['gender'] = df_test['gender'].map({'Female':1, 'Male':2})
df_test['student'] = df_test['student'].map({'Yes':1, 'No':2})
df_test['employment'] = df_test['employment'].map({'Self Employed':1, 'Salaried':2})
df_test.head()

#calculate tenure in months
df_train_= df_train.tenure.str.extractall('(\d+)').unstack()
df_train_.columns = df_train_.columns.droplevel(0)
df_train['tenure'] = df_train_.iloc[:,0].astype(int).mul(12) + df_train_.iloc[:,1].astype(int)
df_train.head()

df_test_= df_test.tenure.str.extractall('(\d+)').unstack()
df_test_.columns = df_test_.columns.droplevel(0)
df_test['tenure'] = df_test_.iloc[:,0].astype(int).mul(12) + df_test_.iloc[:,1].astype(int)
df_test.head()

numerikal = ['age','balance','income','tenure','default']
kategorikal = ['gender','phone_flag','student','employment','credit_card','default']

"""Data visualizatoin"""

#create correlation with hitmap

#create correlation
#corr = df_train.corr(method = 'pearson')
corr = df_train[kategorikal].corr(method = 'spearman')

#convert correlation to numpy array
mask = np.array(corr)

#to mask the repetitive value for each pair
mask[np.tril_indices_from(mask)] = False
fig, ax = plt.subplots(figsize = (7,6))
fig.set_size_inches(10,10)
sns.heatmap(corr, mask = mask, vmax = 0.9, square = True, annot = True)

#create correlation with hitmap

#create correlation
corr = df_train[numerikal].corr(method = 'pearson')
#corr = df_train[kategorikal].corr(method = 'spearman')

#convert correlation to numpy array
mask = np.array(corr)

#to mask the repetitive value for each pair
mask[np.tril_indices_from(mask)] = False
fig, ax = plt.subplots(figsize = (7,6))
fig.set_size_inches(10,10)
sns.heatmap(corr, mask = mask, vmax = 0.9, square = True, annot = True)

sns.boxplot(df_train['age'])

sns.boxplot(df_train['student'])

sns.boxplot(df_train['employment'])

sns.boxplot(df_train['balance'])

"""Drop columns and split the data train"""

df_cek = df_train[['age','student','employment','credit_card','balance','income','tenure','default']]
df_cek_test = df_test[['age','student','employment','credit_card','balance','income','tenure']]

y_trains = df_cek['default']
X_trains = df_cek.drop(columns='default')

# split data
X_train, X_test, y_train, y_test = train_test_split(X_trains, y_trains, test_size=0.33, random_state=42)

"""Creating models with data train

1. k-NN
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
neigh = KNeighborsClassifier(n_neighbors=3)
neigh.fit(X_train, y_train)

y_pred = neigh.predict(X_test)
print(classification_report(y_test, y_pred))
print('accuracy score: ',accuracy_score(y_test,y_pred))

fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
print('auc score: ',metrics.auc(fpr, tpr))

"""2. SVM"""

from sklearn import svm

clf = svm.SVC(gamma=0.001)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))
print('accuracy score: ',accuracy_score(y_test,y_pred))

fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
print('auc score: ',metrics.auc(fpr, tpr))

"""3. Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(random_state=0)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))
print('accuracy score: ',accuracy_score(y_test,y_pred))

fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
print('auc score: ',metrics.auc(fpr, tpr))

"""4. Multinomioal Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB

naive_bayes = MultinomialNB()
naive_bayes.fit(X_train, y_train)

y_pred = naive_bayes.predict(X_test)
print(classification_report(y_test, y_pred))
print('accuracy score: ',accuracy_score(y_test,y_pred))

fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
print('auc score: ',metrics.auc(fpr, tpr))

"""5. Logistic Regression"""

from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial')
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))
print('accuracy score: ',accuracy_score(y_test,y_pred))

fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
print('auc score: ',metrics.auc(fpr, tpr))

"""6. Random Forest"""

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))
print('accuracy score: ',accuracy_score(y_test,y_pred))

fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
print('auc score: ',metrics.auc(fpr, tpr))

"""7. Extra Trees"""

from sklearn.ensemble import ExtraTreesClassifier

clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))
print('accuracy score: ',accuracy_score(y_test,y_pred))

fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
print('auc score: ',metrics.auc(fpr, tpr))

"""Dikarenakan hasil paling bagus dari compare model adalah Decision Tree dengan auc 0.939, maka model tersebut akan dilakukan tuning paramter pada parameter max_depth"""

for i in [3,6,9,12]:
    clf = DecisionTreeClassifier(max_depth=None, min_samples_split=i, random_state=0)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(classification_report(y_test, y_pred))
    print('accuracy score dengan n_estimator = ',i,' : ',accuracy_score(y_test,y_pred))

    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
    print('auc scoredengan n_estimator = ',i,' : ',metrics.auc(fpr, tpr))

"""Dengan dilakukan tuning pada n_estimator, maka didapat n_estimator 12 dengan auc 0.78

Hasil klasifikasi

Dari hasil uji evaluasi, disimpulkan bahawa metode Decision Tree memiliki auc score paling tinggi, yaitu sebanyak 0.78. Maka dari itu, dilakukan pemodelan untuk memprediksi datatest
"""

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=25, max_depth=None, min_samples_split=25, random_state=0)
clf.fit(X_trains, y_trains)

y_pred = clf.predict(df_cek_test)

df_test['default'] = pd.Series(y_pred)

df_test['default'].value_counts()

df_test.head()

df_test_final =  df_test[['customer_id','default']]

df_test_final.head()

df_test_final.to_csv("Hasil Testing Risk Creedit.csv")