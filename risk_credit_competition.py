# -*- coding: utf-8 -*-
"""Risk Credit Competition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/zakiyyah-ai/Risk-Credit-Analyst/blob/main/Risk_Credit_Competition.ipynb
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn import metrics

df_train = pd.read_csv('/content/train.csv')
df_test = pd.read_csv('/content/test.csv')
df_train.head()

"""Data exploratory"""

df_train.shape

print(df_train.info())
df_train.nunique()

df_train.isnull().sum()

#Replace Missing Value
mode_train_gender = df_train['gender'].mode()[0]
mode_train_employment = df_train['employment'].mode()[0]
mode_train_credit_card = df_train['credit_card'].mode()[0]
df_train['gender'] = df_train['gender'].fillna(mode_train_gender)
df_train['employment'] = df_train['employment'].fillna(mode_train_employment)
df_train['credit_card'] = df_train['credit_card'].fillna(mode_train_credit_card)
df_train.isnull().sum()

mode_test_gender = df_test['gender'].mode()[0]
mode_test_employment = df_test['employment'].mode()[0]
mode_test_credit_card = df_test['credit_card'].mode()[0]
df_test['gender'] = df_test['gender'].fillna(mode_test_gender)
df_test['employment'] = df_test['employment'].fillna(mode_test_employment)
df_test['credit_card'] = df_test['credit_card'].fillna(mode_test_credit_card)
df_test = df_test.dropna(axis=0)
df_test.isnull().sum()

#change customer_bod to age
df_train['customer_bod'] = pd.to_datetime(df_train['customer_bod'])
today = pd.to_datetime('today')
df_train['customer_bod'] = today.year - df_train['customer_bod'].dt.year
df_train = df_train.rename(columns = {'customer_bod': 'age'})
df_train.head()

df_test['customer_bod'] = pd.to_datetime(df_test['customer_bod'])
today = pd.to_datetime('today')
df_test['customer_bod'] = today.year - df_test['customer_bod'].dt.year
df_test = df_test.rename(columns = {'customer_bod': 'age'})
df_test.head()

# Create dummy variables, contain 0 or 1 as a result of one hot encoding depending on whether value is True
df_train['gender'] = df_train['gender'].map({'Female':0, 'Male':1})
df_train['student'] = df_train['student'].map({'Yes':0, 'No':1})
df_train['employment'] = df_train['employment'].map({'Self Employed':0, 'Salaried':1})
df_train.head()

df_test['gender'] = df_test['gender'].map({'Female':0, 'Male':1})
df_test['student'] = df_test['student'].map({'Yes':0, 'No':1})
df_test['employment'] = df_test['employment'].map({'Self Employed':0, 'Salaried':1})
df_test.head()

#calculate tenure in months
df_train_= df_train.tenure.str.extractall('(\d+)').unstack()
df_train_.columns = df_train_.columns.droplevel(0)
df_train['tenure'] = df_train_.iloc[:,0].astype(int).mul(12) + df_train_.iloc[:,1].astype(int)
df_train.head()

df_test_= df_test.tenure.str.extractall('(\d+)').unstack()
df_test_.columns = df_test_.columns.droplevel(0)
df_test['tenure'] = df_test_.iloc[:,0].astype(int).mul(12) + df_test_.iloc[:,1].astype(int)
df_test.head()

"""Data visualizatoin"""

pearsoncorr = df_train.corr(method = 'pearson')
fig, ax = plt.subplots(figsize = (7,5))
fig.set_size_inches(10,10)
sns.heatmap(pearsoncorr, 
            xticklabels = pearsoncorr.columns,
            yticklabels = pearsoncorr.columns,
            cmap = 'RdBu_r',
            annot = True,
            linewidth = 0.5)

df_cek = df_train[['age','student','credit_card','balance','income','default']]
df_cek_test = df_test[['age','student','credit_card','balance','income']]

y_trains = df_cek['default']
X_trains = df_cek.drop(columns='default')

# split data
X_train, X_test, y_train, y_test = train_test_split(X_trains, y_trains, test_size=0.33, random_state=42)

"""Creating models with data train

1. k-NN
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
neigh = KNeighborsClassifier(n_neighbors=3)
neigh.fit(X_train, y_train)

y_pred = neigh.predict(X_test)
print(classification_report(y_test, y_pred))
print('accuracy score: ',accuracy_score(y_test,y_pred))

fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
print('auc score: ',metrics.auc(fpr, tpr))

"""2. SVM"""

from sklearn import svm

clf = svm.SVC(gamma=0.001)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))
print('accuracy score: ',accuracy_score(y_test,y_pred))

fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
print('auc score: ',metrics.auc(fpr, tpr))

"""3. Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(random_state=0)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))
print('accuracy score: ',accuracy_score(y_test,y_pred))

fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
print('auc score: ',metrics.auc(fpr, tpr))

"""4. Multinomioal Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB

naive_bayes = MultinomialNB()
naive_bayes.fit(X_train, y_train)

y_pred = naive_bayes.predict(X_test)
print(classification_report(y_test, y_pred))
print('accuracy score: ',accuracy_score(y_test,y_pred))

fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
print('auc score: ',metrics.auc(fpr, tpr))

"""5. Logistic Regression"""

from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial')
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))
print('accuracy score: ',accuracy_score(y_test,y_pred))

fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
print('auc score: ',metrics.auc(fpr, tpr))

"""6. Random Forest"""

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))
print('accuracy score: ',accuracy_score(y_test,y_pred))

fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
print('auc score: ',metrics.auc(fpr, tpr))

"""7. Extra Trees"""

from sklearn.ensemble import ExtraTreesClassifier

clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))
print('accuracy score: ',accuracy_score(y_test,y_pred))

fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
print('auc score: ',metrics.auc(fpr, tpr))

"""Dikarenakan hasil paling bagus dari compare model adalah  Random Forest dengan auc 0.754, maka model tersebut akan dilakukan runing parameter pada parameter max_depth"""

for i in [4,8,12]:
    clf = RandomForestClassifier(max_depth=i, random_state=0)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(classification_report(y_test, y_pred))
    print('accuracy score dengan max_depth = ',i,' : ',accuracy_score(y_test,y_pred))

    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
    print('auc score dengan max_depth = ',i,' : ',metrics.auc(fpr, tpr))

"""Dengan dilakukan runing pada max_depth, maka didapat max_depth 12 dengan auc 0.761

Hasil klasifikasi

Dari hasil uji evaluasi, disimpulkan bahawa metode Random Forest memiliki auc score paling tinggi, yaitu sebanyak 0.761 dan accuracy F1-score sebanyak 0.96. Maka dari itu, dilakukan pemodelan untuk memprediksi datatest
"""

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=15, max_depth=None, min_samples_split=15, random_state=0)
clf.fit(X_trains, y_trains)

y_pred = clf.predict(df_cek_test)

df_test['default'] = pd.Series(y_pred)

df_test['default'].value_counts()

df_test.head()

df_test_final =  df_test[['customer_id','default']]

df_test_final.head()

df_test_final.to_csv("Hasil Testing Risk Creedit.csv")

